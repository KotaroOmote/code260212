{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_make_dataset_colab\n",
    "\n",
    "このノートブックは、7クラス（アナグマ・アライグマ・ハクビシン・タヌキ・ネコ・ノウサギ・テン）分類用データセットの作成を目的としています。\n",
    "\n",
    "実施内容:\n",
    "1. iNaturalist API から画像を収集\n",
    "2. 手元の動画からフレームを抽出（任意）\n",
    "3. `metadata/sources.csv` を作成\n",
    "4. `train/val/test` のCSVを作成\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使いやすい公開データソース\n",
    "\n",
    "- iNaturalist API: https://api.inaturalist.org/v1/observations\n",
    "- iNaturalist API docs: https://www.inaturalist.org/api\n",
    "- GBIF API docs: https://techdocs.gbif.org/en/openapi/images\n",
    "- LILA BC datasets: https://lila.science/datasets\n",
    "\n",
    "注意:\n",
    "- 画像ライセンスは必ず確認してください\n",
    "- 学習データに使う前にクラス誤りを目視で除去してください\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install requests pandas tqdm scikit-learn opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/code260212\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "META_DIR = PROJECT_ROOT / \"metadata\"\n",
    "for p in [RAW_DIR, META_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASS_TAXA: Dict[str, str] = {\n",
    "    \"アナグマ\": \"Meles anakuma\",\n",
    "    \"アライグマ\": \"Procyon lotor\",\n",
    "    \"ハクビシン\": \"Paguma larvata\",\n",
    "    \"タヌキ\": \"Nyctereutes viverrinus\",\n",
    "    \"ネコ\": \"Felis catus\",\n",
    "    \"ノウサギ\": \"Lepus brachyurus\",\n",
    "    \"テン\": \"Martes melampus\",\n",
    "}\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"RAW_DIR: {RAW_DIR}\")\n",
    "print(f\"classes: {list(CLASS_TAXA.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INAT_ENDPOINT = \"https://api.inaturalist.org/v1/observations\"\n",
    "PHOTO_LICENSE = \"cc0,cc-by,cc-by-sa,cc-by-nc\"\n",
    "\n",
    "def _safe_name(text: str) -> str:\n",
    "    return re.sub(r\"[^0-9A-Za-z._-]+\", \"_\", str(text))\n",
    "\n",
    "def _inat_large_url(photo: dict) -> str:\n",
    "    url = photo.get(\"url\", \"\")\n",
    "    if \"/square.\" in url:\n",
    "        return url.replace(\"/square.\", \"/large.\")\n",
    "    return url.replace(\"square\", \"large\")\n",
    "\n",
    "def _download_image(url: str, out_path: Path, timeout: int = 30) -> bool:\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def collect_inat_images(\n",
    "    class_name: str,\n",
    "    taxon_name: str,\n",
    "    max_images: int = 300,\n",
    "    max_pages: int = 15,\n",
    "    per_page: int = 200,\n",
    "    place_id: str = None,\n",
    ") -> List[dict]:\n",
    "    rows: List[dict] = []\n",
    "    out_dir = RAW_DIR / class_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    session = requests.Session()\n",
    "    downloaded = 0\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            \"taxon_name\": taxon_name,\n",
    "            \"quality_grade\": \"research\",\n",
    "            \"photos\": \"true\",\n",
    "            \"photo_license\": PHOTO_LICENSE,\n",
    "            \"per_page\": min(per_page, 200),\n",
    "            \"page\": page,\n",
    "        }\n",
    "        if place_id:\n",
    "            params[\"place_id\"] = place_id\n",
    "\n",
    "        resp = session.get(INAT_ENDPOINT, params=params, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        results = resp.json().get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for obs in results:\n",
    "            obs_id = obs.get(\"id\")\n",
    "            obs_url = obs.get(\"uri\")\n",
    "            observed_on = obs.get(\"observed_on\")\n",
    "            for photo in obs.get(\"photos\", []):\n",
    "                if downloaded >= max_images:\n",
    "                    break\n",
    "                photo_id = photo.get(\"id\")\n",
    "                license_code = photo.get(\"license_code\")\n",
    "                image_url = _inat_large_url(photo)\n",
    "                file_name = _safe_name(f\"inat_{obs_id}_{photo_id}.jpg\")\n",
    "                local_path = out_dir / file_name\n",
    "                ok = local_path.exists() or _download_image(image_url, local_path)\n",
    "                if not ok:\n",
    "                    continue\n",
    "                downloaded += 1\n",
    "                rows.append({\n",
    "                    \"class_name\": class_name,\n",
    "                    \"taxon_name\": taxon_name,\n",
    "                    \"source_dataset\": \"iNaturalist\",\n",
    "                    \"observation_id\": obs_id,\n",
    "                    \"photo_id\": photo_id,\n",
    "                    \"observed_on\": observed_on,\n",
    "                    \"observation_url\": obs_url,\n",
    "                    \"source_url\": image_url,\n",
    "                    \"license_code\": license_code,\n",
    "                    \"file_path\": str(local_path.relative_to(PROJECT_ROOT)),\n",
    "                })\n",
    "            if downloaded >= max_images:\n",
    "                break\n",
    "\n",
    "        if downloaded >= max_images:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IMAGES_PER_CLASS = 300\n",
    "MAX_PAGES = 15\n",
    "PLACE_ID = None  # 日本限定にしたい場合は iNat の place_id を指定\n",
    "\n",
    "all_rows = []\n",
    "for class_name, taxon_name in tqdm(CLASS_TAXA.items(), desc=\"collect classes\"):\n",
    "    rows = collect_inat_images(\n",
    "        class_name=class_name,\n",
    "        taxon_name=taxon_name,\n",
    "        max_images=MAX_IMAGES_PER_CLASS,\n",
    "        max_pages=MAX_PAGES,\n",
    "        place_id=PLACE_ID,\n",
    "    )\n",
    "    all_rows.extend(rows)\n",
    "    print(f\"{class_name}: {len(rows)} images\")\n",
    "\n",
    "inat_df = pd.DataFrame(all_rows)\n",
    "inat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frames_from_video(video_path: Path, out_dir: Path, every_n_frames: int = 10, max_frames: int = 200):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"failed to open: {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    frame_idx = 0\n",
    "    saved = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % every_n_frames == 0:\n",
    "            out_name = f\"{video_path.stem}_f{frame_idx:06d}.jpg\"\n",
    "            out_path = out_dir / out_name\n",
    "            cv2.imwrite(str(out_path), frame)\n",
    "            saved += 1\n",
    "            if saved >= max_frames:\n",
    "                break\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "# 使い方例（必要なら有効化）:\n",
    "# saved = extract_frames_from_video(\n",
    "#     video_path=PROJECT_ROOT / \"data\" / \"videos\" / \"sample.mp4\",\n",
    "#     out_dir=RAW_DIR / \"タヌキ\",\n",
    "#     every_n_frames=8,\n",
    "#     max_frames=300,\n",
    "# )\n",
    "# print(\"saved:\", saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_csv = META_DIR / \"sources.csv\"\n",
    "\n",
    "if sources_csv.exists() and sources_csv.stat().st_size > 0:\n",
    "    old_df = pd.read_csv(sources_csv)\n",
    "else:\n",
    "    old_df = pd.DataFrame()\n",
    "\n",
    "all_df = pd.concat([old_df, inat_df], ignore_index=True, sort=False)\n",
    "if not all_df.empty:\n",
    "    all_df = all_df.drop_duplicates(subset=[\"file_path\"], keep=\"last\")\n",
    "all_df.to_csv(sources_csv, index=False)\n",
    "print(f\"saved: {sources_csv}\")\n",
    "print(all_df[\"class_name\"].value_counts(dropna=False))\n",
    "\n",
    "def make_split(df: pd.DataFrame, test_size: float = 0.15, val_size: float = 0.15, seed: int = 42):\n",
    "    x = df.copy().dropna(subset=[\"class_name\", \"file_path\"]).reset_index(drop=True)\n",
    "    if x.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        train_val, test = train_test_split(\n",
    "            x, test_size=test_size, random_state=seed, stratify=x[\"class_name\"]\n",
    "        )\n",
    "        val_ratio_in_trainval = val_size / (1.0 - test_size)\n",
    "        train, val = train_test_split(\n",
    "            train_val,\n",
    "            test_size=val_ratio_in_trainval,\n",
    "            random_state=seed,\n",
    "            stratify=train_val[\"class_name\"],\n",
    "        )\n",
    "    except ValueError:\n",
    "        # クラス画像が少なすぎる場合はランダム分割にフォールバック\n",
    "        x = x.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "        n = len(x)\n",
    "        n_test = int(n * test_size)\n",
    "        n_val = int(n * val_size)\n",
    "        test = x.iloc[:n_test]\n",
    "        val = x.iloc[n_test:n_test + n_val]\n",
    "        train = x.iloc[n_test + n_val:]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "train_df, val_df, test_df = make_split(all_df)\n",
    "train_df.to_csv(META_DIR / \"train.csv\", index=False)\n",
    "val_df.to_csv(META_DIR / \"val.csv\", index=False)\n",
    "test_df.to_csv(META_DIR / \"test.csv\", index=False)\n",
    "\n",
    "print(\"train/val/test:\", len(train_df), len(val_df), len(test_df))\n",
    "print(\"saved:\", META_DIR / \"train.csv\")\n",
    "print(\"saved:\", META_DIR / \"val.csv\")\n",
    "print(\"saved:\", META_DIR / \"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
